{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bhTU8Tb1ZGWYEJXoyg_c7EN_ZoDeBwmb","timestamp":1679290158027},{"file_id":"1NOULIod93elWHTKCzEIG-MlDpsz8IJld","timestamp":1679224278454}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"3b1b275189b449c98a456324fc8f4d43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ac11baaf402403da115ee81df6ff887","IPY_MODEL_6a27a9e49ced467681e5c31a1b046ed1","IPY_MODEL_127249d0fc1e4d1e9a77a9ffffb06e0b"],"layout":"IPY_MODEL_c915458298124827b692145cd1c23cd2"}},"3ac11baaf402403da115ee81df6ff887":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7237deae641d41f58b60ce0a99003ce4","placeholder":"​","style":"IPY_MODEL_985ee62f2f9041449b18425f74dbba2e","value":"Downloading (…)okenizer_config.json: 100%"}},"6a27a9e49ced467681e5c31a1b046ed1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5abdf336ac04499a0766aa1df465d91","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c426fea38aab4afe9ce917762501395c","value":28}},"127249d0fc1e4d1e9a77a9ffffb06e0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bd4f26cd1c94895924b603bf25c3585","placeholder":"​","style":"IPY_MODEL_de3106f4bb9249429556e907f8a9eda8","value":" 28.0/28.0 [00:00&lt;00:00, 318B/s]"}},"c915458298124827b692145cd1c23cd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7237deae641d41f58b60ce0a99003ce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"985ee62f2f9041449b18425f74dbba2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5abdf336ac04499a0766aa1df465d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c426fea38aab4afe9ce917762501395c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bd4f26cd1c94895924b603bf25c3585":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de3106f4bb9249429556e907f8a9eda8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1445c370416046fdbca8d2453fb4621d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9e1d634fcd84ddc869d899952293250","IPY_MODEL_4cedf6c4d20f46fbae6a0f92e6fbdaa4","IPY_MODEL_32275c5ed59449e6baf89bd8a82383a6"],"layout":"IPY_MODEL_e62310571b244852b84ae7f1cbda28e1"}},"d9e1d634fcd84ddc869d899952293250":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0448853c9f8a41bb97f048e9b9c501ad","placeholder":"​","style":"IPY_MODEL_860fd5755f5b4c9688b15d39e190d49e","value":"Downloading (…)lve/main/config.json: 100%"}},"4cedf6c4d20f46fbae6a0f92e6fbdaa4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3eef7903569749309b1d0694f7d246ef","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ff6ff29adf14d508ebbc11dd1a9473c","value":570}},"32275c5ed59449e6baf89bd8a82383a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f72500ce1e614105aba47a4e497c4138","placeholder":"​","style":"IPY_MODEL_2228d59bf928473c8541e90429e945d4","value":" 570/570 [00:00&lt;00:00, 7.03kB/s]"}},"e62310571b244852b84ae7f1cbda28e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0448853c9f8a41bb97f048e9b9c501ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"860fd5755f5b4c9688b15d39e190d49e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3eef7903569749309b1d0694f7d246ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ff6ff29adf14d508ebbc11dd1a9473c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f72500ce1e614105aba47a4e497c4138":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2228d59bf928473c8541e90429e945d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8afd39c42e543ac88977241ac1481bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddeb05f5b41a41c09139b5f042ee5162","IPY_MODEL_c9b5a063e2d94228b33f8b72996b771b","IPY_MODEL_e0ddc4ed78bf4a1e93584ef7fc82df86"],"layout":"IPY_MODEL_09162ac74e524b8c93aeca69b94a4aef"}},"ddeb05f5b41a41c09139b5f042ee5162":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63af446f0bcf4b73ba1c070f114f0520","placeholder":"​","style":"IPY_MODEL_004465c7f1224e9bb554e245cb2af4d6","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"c9b5a063e2d94228b33f8b72996b771b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_145ca1bcbedd449fbad5909b947aeba9","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8929bd78e57e4934ba303a15751bdf05","value":231508}},"e0ddc4ed78bf4a1e93584ef7fc82df86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89e97f1d0ff049abae7a8a8cd88c3776","placeholder":"​","style":"IPY_MODEL_01f0636f86a34b10913fd425cc0c2f42","value":" 232k/232k [00:00&lt;00:00, 1.89MB/s]"}},"09162ac74e524b8c93aeca69b94a4aef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63af446f0bcf4b73ba1c070f114f0520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"004465c7f1224e9bb554e245cb2af4d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"145ca1bcbedd449fbad5909b947aeba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8929bd78e57e4934ba303a15751bdf05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89e97f1d0ff049abae7a8a8cd88c3776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01f0636f86a34b10913fd425cc0c2f42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"665223fb89434690a84a9a6f34b615be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9739f8e83c8a46d586ae1ebb2e7a9048","IPY_MODEL_1ea13319cfbe4dba83bebdc88c2cb321","IPY_MODEL_ee63790e3fa54c958e18c289897e7c6a"],"layout":"IPY_MODEL_51d1c3afdce44c2face8e97003b6f26c"}},"9739f8e83c8a46d586ae1ebb2e7a9048":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bce76ed807c5458c969e1f3d9dfba7cb","placeholder":"​","style":"IPY_MODEL_68ae70321ee7412bb20578d8ea18b036","value":"Downloading (…)/main/tokenizer.json: 100%"}},"1ea13319cfbe4dba83bebdc88c2cb321":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5203535c2e024dd999a17c30773190a4","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c91d5842f654d34bd8f128b1b3c0a52","value":466062}},"ee63790e3fa54c958e18c289897e7c6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b41a092f23cf4aff919d4ff9c6744c78","placeholder":"​","style":"IPY_MODEL_8bda2bb7fb4b48378d54a74f6a8d16f1","value":" 466k/466k [00:00&lt;00:00, 3.18MB/s]"}},"51d1c3afdce44c2face8e97003b6f26c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bce76ed807c5458c969e1f3d9dfba7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ae70321ee7412bb20578d8ea18b036":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5203535c2e024dd999a17c30773190a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c91d5842f654d34bd8f128b1b3c0a52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b41a092f23cf4aff919d4ff9c6744c78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bda2bb7fb4b48378d54a74f6a8d16f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5ebb3d0b26548f5b967b2d5d2d31837":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2a2b0727033424e86b413d42f869ca0","IPY_MODEL_b98a076f5ca44520b6fdc554645fa1c3","IPY_MODEL_ed88535a49aa43b4aa299f469454b6ce"],"layout":"IPY_MODEL_901f7948464247958b830649ef1f392c"}},"c2a2b0727033424e86b413d42f869ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ef70ecd936d4dd2a17c69ba706fa679","placeholder":"​","style":"IPY_MODEL_f475770c0f054842b38cd72af9570b67","value":"Downloading pytorch_model.bin: 100%"}},"b98a076f5ca44520b6fdc554645fa1c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc9a749d59bc4bceace48a8e7a1a6d3a","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b596e704496647c7b494b998a4a417ba","value":440473133}},"ed88535a49aa43b4aa299f469454b6ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e26a40ccde045fdb6c02e0188559153","placeholder":"​","style":"IPY_MODEL_95662244547049bebd84277ac4b138eb","value":" 440M/440M [00:03&lt;00:00, 134MB/s]"}},"901f7948464247958b830649ef1f392c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef70ecd936d4dd2a17c69ba706fa679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f475770c0f054842b38cd72af9570b67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc9a749d59bc4bceace48a8e7a1a6d3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b596e704496647c7b494b998a4a417ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e26a40ccde045fdb6c02e0188559153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95662244547049bebd84277ac4b138eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcCALF9iaRHJ","executionInfo":{"status":"ok","timestamp":1679396743012,"user_tz":-330,"elapsed":72784,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"outputId":"1809d6f1-61de-44fd-ce52-5902a485858a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install multimodal-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KacNDnPCIz4g","executionInfo":{"status":"ok","timestamp":1679396807681,"user_tz":-330,"elapsed":43595,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"outputId":"b2d0dec3-6cb0-4a48-cb8b-b0160adea3a0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting multimodal-transformers\n","  Downloading multimodal_transformers-0.2a0-py3-none-any.whl (22 kB)\n","Collecting networkx~=2.6.3\n","  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn~=1.0.2\n","  Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm~=4.64.1\n","  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses~=0.0.53\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pandas~=1.3.5\n","  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy~=1.7.3\n","  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.8/39.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers>=4.26.1\n","  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.9/dist-packages (from multimodal-transformers) (1.13.1+cu116)\n","Collecting numpy~=1.21.6\n","  Downloading numpy-1.21.6-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytest~=7.2.2\n","  Downloading pytest-7.2.2-py3-none-any.whl (317 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 KB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->multimodal-transformers) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->multimodal-transformers) (2022.7.1)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest~=7.2.2->multimodal-transformers) (2.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytest~=7.2.2->multimodal-transformers) (23.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest~=7.2.2->multimodal-transformers) (22.2.0)\n","Collecting pluggy<2.0,>=0.12\n","  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n","Collecting iniconfig\n","  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n","Collecting exceptiongroup>=1.0.0rc8\n","  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacremoses~=0.0.53->multimodal-transformers) (2022.10.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses~=0.0.53->multimodal-transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses~=0.0.53->multimodal-transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses~=0.0.53->multimodal-transformers) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn~=1.0.2->multimodal-transformers) (3.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.1->multimodal-transformers) (4.5.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.26.1->multimodal-transformers) (3.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.26.1->multimodal-transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=4.26.1->multimodal-transformers) (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.26.1->multimodal-transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.26.1->multimodal-transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.26.1->multimodal-transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=4.26.1->multimodal-transformers) (2.0.12)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=2334c3d1e0d2a677e1367ca69b74c5f997d5e4a95cbd2574a6e7b3b580570126\n","  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, tqdm, pluggy, numpy, networkx, iniconfig, exceptiongroup, scipy, sacremoses, pytest, pandas, huggingface-hub, transformers, scikit-learn, multimodal-transformers\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.65.0\n","    Uninstalling tqdm-4.65.0:\n","      Successfully uninstalled tqdm-4.65.0\n","  Attempting uninstall: pluggy\n","    Found existing installation: pluggy 0.7.1\n","    Uninstalling pluggy-0.7.1:\n","      Successfully uninstalled pluggy-0.7.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.0\n","    Uninstalling networkx-3.0:\n","      Successfully uninstalled networkx-3.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: pytest\n","    Found existing installation: pytest 3.6.4\n","    Uninstalling pytest-3.6.4:\n","      Successfully uninstalled pytest-3.6.4\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.4.4\n","    Uninstalling pandas-1.4.4:\n","      Successfully uninstalled pandas-1.4.4\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed exceptiongroup-1.1.1 huggingface-hub-0.13.3 iniconfig-2.0.0 multimodal-transformers-0.2a0 networkx-2.6.3 numpy-1.21.6 pandas-1.3.5 pluggy-1.0.0 pytest-7.2.2 sacremoses-0.0.53 scikit-learn-1.0.2 scipy-1.7.3 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.27.2\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import json\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import AutoModelWithTabular\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"K00AZB93I0eN","executionInfo":{"status":"ok","timestamp":1679396824726,"user_tz":-330,"elapsed":11070,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JgvZVR-gN5RK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","if torch.cuda.is_available():    \n","    print(\"GPU Available, using GPU \\n\")\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVanjaXTcIgw","executionInfo":{"status":"ok","timestamp":1679396825537,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"outputId":"34d7fd7a-5004-47e9-8116-fbeb185d7705"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Available, using GPU \n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","dataset_dir_path = 'drive/MyDrive/BTP/intentconan.csv'\n","columns = [\"CounterSpeech\", \"HateSpeech\" , \"Relevance\"]\n","data_df = pd.read_csv(dataset_dir_path , usecols = columns)\n","# train_df['Aggressive'] = train_df['Aggressive'].apply(lambda x: x-1)\n","data_df['Relevance'] = data_df['Relevance'].apply(lambda x: x-1)\n","data_df.dropna(inplace=True)\n","data_df['Relevance'] = data_df['Relevance'].apply(lambda x: 1.0 if x == 2.0 or x == 3.0 else (2.0 if x == 4.0 or x == 5.0 else 0.0))\n","data_df.head(100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Y6yuM6SwcOKO","executionInfo":{"status":"ok","timestamp":1679396825537,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"outputId":"bb770ad2-227f-4b16-9452-0cf39bcf7fc4"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                           HateSpeech  \\\n","0   Maybe the UN could talk to those asian and afr...   \n","1   Maybe the UN could talk to those asian and afr...   \n","2   Maybe the UN could talk to those asian and afr...   \n","3   Maybe the UN could talk to those asian and afr...   \n","4   ‘Juice (Jews) are worse that nukes.’ Caption t...   \n","..                                                ...   \n","95  Pakistan: 202 Million Nigeria: 193 Million Ban...   \n","96  Took this in Wanstead, east London. An area wi...   \n","97  Took this in Wanstead, east London. An area wi...   \n","98  Took this in Wanstead, east London. An area wi...   \n","99  Took this in Wanstead, east London. An area wi...   \n","\n","                                        CounterSpeech  Relevance  \n","0   The US is the second most polluting country in...        1.0  \n","1   Doesn't everyone on the planet have a responsi...        1.0  \n","2   The world would be a better place if people we...        0.0  \n","3   You're right, ocean pollution is one of the ma...        1.0  \n","4   Anti-semitism is a serious problem that we nee...        1.0  \n","..                                                ...        ...  \n","95  These groups tend to be a minority in Britian....        1.0  \n","96  There are more than enough empty homes in this...        2.0  \n","97  Is it due to immigration, or the lack of affor...        1.0  \n","98  There are so many empty homes in our country t...        2.0  \n","99  Asylum seekers and refugees deserve our compas...        1.0  \n","\n","[100 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-dd5a8e71-5d4a-497a-a863-f79bae69658d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HateSpeech</th>\n","      <th>CounterSpeech</th>\n","      <th>Relevance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Maybe the UN could talk to those asian and afr...</td>\n","      <td>The US is the second most polluting country in...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Maybe the UN could talk to those asian and afr...</td>\n","      <td>Doesn't everyone on the planet have a responsi...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Maybe the UN could talk to those asian and afr...</td>\n","      <td>The world would be a better place if people we...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Maybe the UN could talk to those asian and afr...</td>\n","      <td>You're right, ocean pollution is one of the ma...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>‘Juice (Jews) are worse that nukes.’ Caption t...</td>\n","      <td>Anti-semitism is a serious problem that we nee...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Pakistan: 202 Million Nigeria: 193 Million Ban...</td>\n","      <td>These groups tend to be a minority in Britian....</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>Took this in Wanstead, east London. An area wi...</td>\n","      <td>There are more than enough empty homes in this...</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Took this in Wanstead, east London. An area wi...</td>\n","      <td>Is it due to immigration, or the lack of affor...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Took this in Wanstead, east London. An area wi...</td>\n","      <td>There are so many empty homes in our country t...</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>Took this in Wanstead, east London. An area wi...</td>\n","      <td>Asylum seekers and refugees deserve our compas...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd5a8e71-5d4a-497a-a863-f79bae69658d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dd5a8e71-5d4a-497a-a863-f79bae69658d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dd5a8e71-5d4a-497a-a863-f79bae69658d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data_df.isnull().sum().sum()"],"metadata":{"id":"N3kKSe6Wfyq_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679386190560,"user_tz":-330,"elapsed":42,"user":{"displayName":"Japneet Singh","userId":"10624542182933755587"}},"outputId":"98a1c81f-075d-4c4d-a7a5-2a96a5357b26"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# train_df['Aggressive'].value_counts()\n","data_df['Relevance'].value_counts()"],"metadata":{"id":"8kikL4MdgEYb","executionInfo":{"status":"ok","timestamp":1679386190561,"user_tz":-330,"elapsed":38,"user":{"displayName":"Japneet Singh","userId":"10624542182933755587"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"76224873-558b-4b95-b0ad-4e147a4bd4be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0    3783\n","0.0    1357\n","2.0     977\n","Name: Relevance, dtype: int64"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["counterspeech = data_df['CounterSpeech'].values\n","hatespeech = data_df['HateSpeech'].values\n","rel_labels = data_df['Relevance'].values\n","print(len(rel_labels), len(counterspeech), len(hatespeech))"],"metadata":{"id":"2ULMdsyAgEeO","executionInfo":{"status":"ok","timestamp":1679386190561,"user_tz":-330,"elapsed":35,"user":{"displayName":"Japneet Singh","userId":"10624542182933755587"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9c0db77-7130-451d-b102-58e453cae5ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6117 6117 6117\n"]}]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(data_df.sample(frac=1), [int(.8*len(data_df)), int(.9 * len(data_df))])\n","print('Num examples train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('drive/MyDrive/BTP/train.csv')\n","val_df.to_csv('drive/MyDrive/BTP/val.csv')\n","test_df.to_csv('drive/MyDrive/BTP/test.csv')\n","\n","from google.colab import files\n","\n","# files.download('train.csv')\n","# files.download('val.csv')\n","# files.download('test.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8OaRlWUWXrh","executionInfo":{"status":"ok","timestamp":1679396826358,"user_tz":-330,"elapsed":825,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"outputId":"1e10a965-770c-4469-a330-ba564d390000"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Num examples train-val-test\n","4893 612 612\n"]}]},{"cell_type":"code","source":["@dataclass\n","class ModelArguments:\n","  \"\"\"\n","  Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","  \"\"\"\n","\n","  model_name_or_path: str = field(\n","      metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","  )\n","  config_name: Optional[str] = field(\n","      default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","  )\n","  tokenizer_name: Optional[str] = field(\n","      default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","  )\n","  cache_dir: Optional[str] = field(\n","      default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n","  )\n","\n","\n","@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='ohe',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"classification\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=True,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })\n","\n","  def __post_init__(self):\n","      assert self.column_info != self.column_info_path\n","      if self.column_info is None and self.column_info_path:\n","          with open(self.column_info_path, 'r') as f:\n","              self.column_info = json.load(f)"],"metadata":{"id":"g3ljujrBThgn","executionInfo":{"status":"ok","timestamp":1679396829156,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["text_cols = ['HateSpeech', 'CounterSpeech']\n","# cat_cols = ['Clothing ID', 'Division Name', 'Department Name', 'Class Name']\n","# numerical_cols = ['Relevance']\n","\n","column_info_dict = {\n","    'text_cols': text_cols,\n","    'label_col': 'Relevance',\n","    'label_list': [0.0,1.0,2.0]\n","}\n","\n","\n","model_args = ModelArguments(\n","    model_name_or_path='bert-base-uncased'\n",")\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='drive/MyDrive/BTP/',\n","    column_info=column_info_dict,\n","    task='classification'\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"drive/MyDrive/BTP/logs/model_name\",\n","    logging_dir=\"drive/MyDrive/BTP/logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    do_predict=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=5,\n","    evaluation_strategy = \"steps\",\n","    logging_steps=25,\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"uwSxVAb_UKO2","executionInfo":{"status":"ok","timestamp":1679396830868,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n","print('Specified tokenizer: ', tokenizer_path_or_name)\n","tokenizer = AutoTokenizer.from_pretrained(\n","    tokenizer_path_or_name,\n","    cache_dir=model_args.cache_dir,\n",")"],"metadata":{"id":"XPoQns0-V3Fr","executionInfo":{"status":"ok","timestamp":1679396835670,"user_tz":-330,"elapsed":2864,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"colab":{"base_uri":"https://localhost:8080/","height":163,"referenced_widgets":["3b1b275189b449c98a456324fc8f4d43","3ac11baaf402403da115ee81df6ff887","6a27a9e49ced467681e5c31a1b046ed1","127249d0fc1e4d1e9a77a9ffffb06e0b","c915458298124827b692145cd1c23cd2","7237deae641d41f58b60ce0a99003ce4","985ee62f2f9041449b18425f74dbba2e","f5abdf336ac04499a0766aa1df465d91","c426fea38aab4afe9ce917762501395c","7bd4f26cd1c94895924b603bf25c3585","de3106f4bb9249429556e907f8a9eda8","1445c370416046fdbca8d2453fb4621d","d9e1d634fcd84ddc869d899952293250","4cedf6c4d20f46fbae6a0f92e6fbdaa4","32275c5ed59449e6baf89bd8a82383a6","e62310571b244852b84ae7f1cbda28e1","0448853c9f8a41bb97f048e9b9c501ad","860fd5755f5b4c9688b15d39e190d49e","3eef7903569749309b1d0694f7d246ef","9ff6ff29adf14d508ebbc11dd1a9473c","f72500ce1e614105aba47a4e497c4138","2228d59bf928473c8541e90429e945d4","e8afd39c42e543ac88977241ac1481bd","ddeb05f5b41a41c09139b5f042ee5162","c9b5a063e2d94228b33f8b72996b771b","e0ddc4ed78bf4a1e93584ef7fc82df86","09162ac74e524b8c93aeca69b94a4aef","63af446f0bcf4b73ba1c070f114f0520","004465c7f1224e9bb554e245cb2af4d6","145ca1bcbedd449fbad5909b947aeba9","8929bd78e57e4934ba303a15751bdf05","89e97f1d0ff049abae7a8a8cd88c3776","01f0636f86a34b10913fd425cc0c2f42","665223fb89434690a84a9a6f34b615be","9739f8e83c8a46d586ae1ebb2e7a9048","1ea13319cfbe4dba83bebdc88c2cb321","ee63790e3fa54c958e18c289897e7c6a","51d1c3afdce44c2face8e97003b6f26c","bce76ed807c5458c969e1f3d9dfba7cb","68ae70321ee7412bb20578d8ea18b036","5203535c2e024dd999a17c30773190a4","8c91d5842f654d34bd8f128b1b3c0a52","b41a092f23cf4aff919d4ff9c6744c78","8bda2bb7fb4b48378d54a74f6a8d16f1"]},"outputId":"48e241f0-0fb6-4ec7-d46b-6450fd608d7c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  bert-base-uncased\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1b275189b449c98a456324fc8f4d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1445c370416046fdbca8d2453fb4621d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8afd39c42e543ac88977241ac1481bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"665223fb89434690a84a9a6f34b615be"}},"metadata":{}}]},{"cell_type":"code","source":["data_df.columns"],"metadata":{"id":"QjnDuJGugddU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    folder_path = data_args.data_path,\n","    text_cols = data_args.column_info['text_cols'],\n","    tokenizer = tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list=data_args.column_info['label_list'],\n","    categorical_cols=None,\n","    numerical_cols=None,\n","    categorical_encode_type=None,\n","    numerical_transformer_method='none',\n","    sep_text_token_str=tokenizer.sep_token,\n",")"],"metadata":{"id":"mMTQtZVgV3H7","executionInfo":{"status":"ok","timestamp":1679396838286,"user_tz":-330,"elapsed":2618,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["num_labels = len(np.unique(train_dataset.labels))\n","num_labels"],"metadata":{"id":"30w1DmLEWAQv","executionInfo":{"status":"ok","timestamp":1679396839186,"user_tz":-330,"elapsed":902,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"35492803-20a0-4562-e201-9d997aa667d2"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        cache_dir=model_args.cache_dir,\n","    )\n","tabular_config = TabularConfig(num_labels=num_labels,\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"bWfWFdtsWATG","executionInfo":{"status":"ok","timestamp":1679396839597,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model = AutoModelWithTabular.from_pretrained(\n","        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","        config=config,\n","        cache_dir=model_args.cache_dir\n","    )"],"metadata":{"id":"bCKYWei2WAYb","executionInfo":{"status":"ok","timestamp":1679396848673,"user_tz":-330,"elapsed":8597,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["f5ebb3d0b26548f5b967b2d5d2d31837","c2a2b0727033424e86b413d42f869ca0","b98a076f5ca44520b6fdc554645fa1c3","ed88535a49aa43b4aa299f469454b6ce","901f7948464247958b830649ef1f392c","7ef70ecd936d4dd2a17c69ba706fa679","f475770c0f054842b38cd72af9570b67","fc9a749d59bc4bceace48a8e7a1a6d3a","b596e704496647c7b494b998a4a417ba","3e26a40ccde045fdb6c02e0188559153","95662244547049bebd84277ac4b138eb"]},"outputId":"421234a1-6f37-4411-81cb-f8700b414020"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ebb3d0b26548f5b967b2d5d2d31837"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertWithTabular: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertWithTabular were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['tabular_classifier.bias', 'tabular_classifier.weight', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["item = [[-1.9141968,  -0.486645,   -1.5046495,   3.447974,    0.07784579],[-2.1495693,   0.1755497,  -1.0876646 ,  3.2793176,  -0.6566321 ]]\n","print(np.argmax(item, axis=1))"],"metadata":{"id":"1M5QpCwXvC9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aR2f0hFPvpgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.special import softmax\n","from sklearn.metrics import (\n","    auc,\n","    precision_recall_curve,\n","    roc_auc_score,\n","    f1_score,\n","    confusion_matrix,\n","    matthews_corrcoef,\n",")\n","\n","def calc_classification_metrics(p: EvalPrediction):\n","  # print(p.predictions[0])\n","  # print(len(p.predictions[0]))\n","  # print('-0-------------------')\n","  # print(p.predictions[1], type(p.predictions[0]), type(p.predictions[1]), len(p.predictions), type(p.predictions))\n","\n","  # for i in range(len(p.predictions)):\n","  #   p.predictions[i] = torch.from_numpy(np_array)\n","  # pred2 = torch.tensor(p.predictions)\n","  # pred2 = [torch.from_numpy(item).float() for item in p.predictions]\n","  pred_labels = np.argmax(p.predictions[0], axis=1)\n","  # print(pred_labels)\n","  pred_scores = softmax(p.predictions[0], axis=1)\n","  print(pred_scores)\n","  labels = p.label_ids\n","  labels = [int(x) for x in labels]\n","  # print('p.label_ids \\n', p.label_ids)\n","  # if len(np.unique(labels)) == 5: \n","  #     # roc_auc_pred_score = roc_auc_score(labels, pred_scores, multi_class='ovo')\n","  #     precisions, recalls, thresholds = precision_recall_curve(labels,\n","  #                                                               pred_scores)\n","  #     fscore = (2 * precisions * recalls) / (precisions + recalls)\n","  #     fscore[np.isnan(fscore)] = 0\n","  #     ix = np.argmax(fscore)\n","  #     threshold = thresholds[ix].item()\n","  #     # pr_auc = auc(recalls, precisions)\n","  #     tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0.0, 1.0, 2.0, 3.0, 4.0]).ravel()\n","  #     result = {\n","  #         'threshold': threshold,\n","  #               'recall': recalls[ix].item(),\n","  #               'precision': precisions[ix].item(), 'f1': fscore[ix].item(),\n","  #               'tn': tn.item(), 'fp': fp.item(), 'fn': fn.item(), 'tp': tp.item()\n","  #               }\n","  # else:\n","  acc = (pred_labels == labels).mean()\n","  f1 = f1_score(y_true=labels, y_pred=pred_labels, average=\"macro\")\n","  result = {\n","      \"acc\": acc,\n","      \"f1\": f1,\n","      \"acc_and_f1\": (acc + f1) / 2,\n","      \"mcc\": matthews_corrcoef(labels, pred_labels)\n","  }\n","\n","  return result"],"metadata":{"id":"lPrfF6imWAa_","executionInfo":{"status":"ok","timestamp":1679396849327,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_classification_metrics,\n",")"],"metadata":{"id":"v2ePdEPdWAdl","executionInfo":{"status":"ok","timestamp":1679396858199,"user_tz":-330,"elapsed":8876,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"wJut73m_kaRd","executionInfo":{"status":"ok","timestamp":1679397045151,"user_tz":-330,"elapsed":186964,"user":{"displayName":"Ayush Goyal","userId":"16886632502915947840"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"87d0a8be-18a8-42f0-8faf-50b8a3be06b2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='186' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [186/765 03:02 < 09:33, 1.01 it/s, Epoch 1.21/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Acc</th>\n","      <th>F1</th>\n","      <th>Acc And F1</th>\n","      <th>Mcc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>0.944700</td>\n","      <td>0.859832</td>\n","      <td>0.630719</td>\n","      <td>0.257849</td>\n","      <td>0.444284</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.873700</td>\n","      <td>0.812203</td>\n","      <td>0.661765</td>\n","      <td>0.402398</td>\n","      <td>0.532081</td>\n","      <td>0.238571</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.809700</td>\n","      <td>0.835031</td>\n","      <td>0.661765</td>\n","      <td>0.369097</td>\n","      <td>0.515431</td>\n","      <td>0.228486</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.846300</td>\n","      <td>0.812393</td>\n","      <td>0.666667</td>\n","      <td>0.375244</td>\n","      <td>0.520955</td>\n","      <td>0.249708</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.859600</td>\n","      <td>0.781951</td>\n","      <td>0.671569</td>\n","      <td>0.434206</td>\n","      <td>0.552887</td>\n","      <td>0.286181</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.815000</td>\n","      <td>0.797520</td>\n","      <td>0.676471</td>\n","      <td>0.404101</td>\n","      <td>0.540286</td>\n","      <td>0.282111</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.742800</td>\n","      <td>0.798174</td>\n","      <td>0.660131</td>\n","      <td>0.478455</td>\n","      <td>0.569293</td>\n","      <td>0.273810</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[[0.13349433 0.63835996 0.22814573]\n"," [0.14120577 0.68806696 0.17072742]\n"," [0.11724342 0.56049544 0.32226127]\n"," ...\n"," [0.2852675  0.62725234 0.08748017]\n"," [0.12346196 0.5932531  0.28328496]\n"," [0.12436587 0.60352486 0.2721093 ]]\n","[[0.19200978 0.68489397 0.12309613]\n"," [0.17133233 0.6919045  0.13676317]\n"," [0.09561537 0.57865936 0.32572526]\n"," ...\n"," [0.23126824 0.6631783  0.10555338]\n"," [0.11526346 0.64265895 0.24207771]\n"," [0.10541605 0.63141996 0.26316398]]\n","[[0.06239272 0.6790722  0.2585351 ]\n"," [0.08253594 0.76950127 0.14796264]\n"," [0.04599463 0.52344006 0.43056536]\n"," ...\n"," [0.08387084 0.7729898  0.14313936]\n"," [0.05175877 0.6239482  0.32429302]\n"," [0.05132419 0.5896637  0.359012  ]]\n","[[0.07564671 0.74499315 0.17936024]\n"," [0.09066863 0.76486075 0.14447053]\n"," [0.0565259  0.65387917 0.289595  ]\n"," ...\n"," [0.15879396 0.76107764 0.08012843]\n"," [0.09526    0.74997723 0.15476276]\n"," [0.06841934 0.7055052  0.22607544]]\n","[[0.10396561 0.7587283  0.13730595]\n"," [0.13819459 0.729995   0.13181026]\n"," [0.0601905  0.66676563 0.2730439 ]\n"," ...\n"," [0.23247427 0.68383145 0.08369435]\n"," [0.09866497 0.72643125 0.17490377]\n"," [0.06805926 0.7047977  0.22714299]]\n","[[0.08858611 0.6503254  0.26108864]\n"," [0.12479712 0.6726002  0.20260264]\n"," [0.07152476 0.55670667 0.3717685 ]\n"," ...\n"," [0.15437986 0.7544588  0.09116133]\n"," [0.10927963 0.61601204 0.27470827]\n"," [0.07338682 0.5938833  0.33272994]]\n","[[0.07503466 0.74060667 0.18435857]\n"," [0.12632237 0.7497259  0.12395166]\n"," [0.04120723 0.3902628  0.5685299 ]\n"," ...\n"," [0.13832332 0.7903697  0.07130699]\n"," [0.06550392 0.6105246  0.32397148]\n"," [0.04227872 0.46454886 0.49317238]]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2661\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2663\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"metadata":{"id":"SPTHQg-IkaTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir ./logs/runs --port=6006"],"metadata":{"id":"RXvj947EThjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"XP_OCLYimofN"}},{"cell_type":"code","source":[],"metadata":{"id":"YE5NMANomohs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_drACdppmokN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3yjTaB3Qmomn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0AcGZyIcmopI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-rRDdX1Hmorc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2UusmJ7Rmot0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5d94JbtPmowd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZJZ1f5-wmoyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LxdwxPDZmo1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MNsPhszamo3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jrX3JJrLmo5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","\n","dataset = TensorDataset(input_ids, attention_masks, rel_labels)\n","\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"],"metadata":{"id":"0BGMkrTFThaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install transformers"],"metadata":{"id":"NkH6AYjBg6vH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"LZCkeYTEgEgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import matplotlib.pyplot as plt\n","\n","def returnZero():\n","    return 0\n","max_len = 0\n","lengths = defaultdict(returnZero)\n","for cs in counterspeech:\n","    input_ids = tokenizer.encode(str(cs), add_special_tokens=True)\n","    lengths[len(input_ids)]+=1\n","    max_len = max(max_len, len(input_ids))\n","lengths\n","\n","keys=sorted(list(lengths.keys()))\n","vals = [lengths[key] for key in keys]\n","plt.figure(figsize=(20, 20))\n","plt.plot(keys, vals)\n","plt.title('Len vs count')\n","plt.ylabel('count')\n","plt.xlabel('len')\n","plt.savefig('vis.jpg')\n","\n","# post 40-42 the count of title lengths is negligible, to avoid trailing zeroes, setting pad length to 42"],"metadata":{"id":"M1U3RAkpgEjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_pad_length = 120\n","to_pad = True"],"metadata":{"id":"3YWEGfFGgEli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","for cs in counterspeech:\n","    encoded_dict = tokenizer.encode_plus(\n","                        str(cs),                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_pad_length,           # Pad & truncate all sentences.\n","                        pad_to_max_length = to_pad,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","rel_labels = torch.tensor(rel_labels)"],"metadata":{"id":"Y3T1gJbYgEn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_ids)"],"metadata":{"id":"R23nFwmMgEqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","\n","dataset = TensorDataset(input_ids, attention_masks, rel_labels)\n","\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"],"metadata":{"id":"nXfjyYqehQK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 32\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"metadata":{"id":"0NEWRttlgFGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"id":"AvrJClMpgFI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 3, # The number of output labels is 30\n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","model.cuda()"],"metadata":{"id":"M-mmTXFCgUKz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"id":"l7turA0xgUNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","# epochs experimented between 2, 3 and 4 \n","epochs = 2\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, \n","                                            num_training_steps = total_steps)"],"metadata":{"id":"zNE03gTxgUlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    print(pred_flat)\n","    print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"HxvqRepygUn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = torch.tensor([0.,2.,2.,1.,1.,1.,1.])\n","torch.nn.functional.one_hot(test.to(torch.int64), 3)"],"metadata":{"id":"Mm5p6F0wgb4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","seed_val = 66\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","for epoch_i in range(0, epochs):\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","#         b_labels = torch.nn.functional.one_hot(batch[2].to(torch.int64),5)\n","#         print(batch[2].to(torch.int64))\n","        b_labels = batch[2].to(torch.int64).to(device)\n","        model.zero_grad()      \n","#         print(b_labels)\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels,return_dict = False)\n","        total_train_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    for batch in validation_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(torch.int64).to(device)\n","        with torch.no_grad():        \n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels,return_dict = False)\n","        total_eval_loss += loss.item()\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        print('preds',logits)\n","        print('act',label_ids)\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))"],"metadata":{"id":"OFjkD8JIgb7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j2bbI292nBoN"},"execution_count":null,"outputs":[]}]}